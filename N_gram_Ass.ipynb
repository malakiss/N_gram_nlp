{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Process the IMDB Dataset"
      ],
      "metadata": {
        "id": "cD9ZRNeIJ_Br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gdown\n",
        "\n",
        "# 1. Download the zipped IMDB dataset from Drive\n",
        "# this is the unsup part of https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "!gdown \"https://drive.google.com/uc?id=1PjJ5cop0pT6tcEw9-ZUstVMujx-o-QTB\" -O imdb_dataset.zip\n",
        "\n",
        "# 2. Unzip the downloaded file\n",
        "!unzip -q imdb_dataset.zip -d imdb_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0olLNujvMbg8",
        "outputId": "6e54f1c5-aa65-4150-d3e5-dbcc66877ce4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1PjJ5cop0pT6tcEw9-ZUstVMujx-o-QTB\n",
            "From (redirected): https://drive.google.com/uc?id=1PjJ5cop0pT6tcEw9-ZUstVMujx-o-QTB&confirm=t&uuid=1ace5eb1-0959-41aa-bde3-549a43beb9f8\n",
            "To: /content/imdb_dataset.zip\n",
            "100% 44.7M/44.7M [00:00<00:00, 63.0MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "from math import log, exp\n"
      ],
      "metadata": {
        "id": "siDkX864cazJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1K-8ybJJ3Cl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be31db52-177a-4853-a927-5c5a64a353c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "# Ensure stopwords are downloaded\n",
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"wordnet\")\n",
        "\n",
        "CUSTOM_STOPWORDS = {\"the\", \"a\", \"an\", \"is\", \"are\", \"was\", \"were\", \"this\", \"that\", \"to\", \"of\", \"and\", \"on\"}\n",
        "\n",
        "\n",
        "STOP_WORDS = set(stopwords.words(\"english\"))  # Load stopwords\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "def load_imdb_unsup_sentences(folder_path):\n",
        "    \"\"\"\n",
        "    Loads text files from the IMDB 'unsup' (unsupervised) folder.\n",
        "    - Reads all `.txt` files from the given folder.\n",
        "    - Splits text by newline, strips each line, and returns a list of raw lines.\n",
        "    - Replaces <br /> tags with a special token <nl>.\n",
        "    \"\"\"\n",
        "    all_sentences = []\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if os.path.isfile(file_path) and file_name.endswith('.txt'):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if line:\n",
        "                        line = re.sub(r\"<br\\s*/?>\", \" <nl> \", line)  # Replace <br /> with <nl>\n",
        "                        all_sentences.append(line)\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"\n",
        "    Removes punctuation from the text while preserving <nl> tokens.\n",
        "    Also removes apostrophes and stopwords.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"<br\\s*/?>\", \" <nl> \", text)  # Ensure <br /> becomes <nl>\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    regex_pattern = f\"[{re.escape(string.punctuation)}]\"\n",
        "    text = re.sub(regex_pattern, \"\", text)\n",
        "    processed_words = []\n",
        "    for word in text.split():\n",
        "        #if word not in stop_words:\n",
        "            processed_words.append(word)\n",
        "\n",
        "    return \" \".join(processed_words)\n",
        "\n",
        "def build_vocabulary(sentences):\n",
        "\n",
        "    vocab = set()\n",
        "\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = remove_punctuation(sentence.lower())  # Lowercase & clean\n",
        "        tokens = cleaned_sentence.split()\n",
        "        vocab.update(tokens)\n",
        "    return vocab\n",
        "\n",
        "def tokinize(sentences, vocab, unknown=\"<UNK>\"):\n",
        "\n",
        "    tokenized_sentences = []\n",
        "\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = remove_punctuation(sentence.lower())  # Lowercase & clean\n",
        "        tokens = [\n",
        "            token\n",
        "            if token in vocab\n",
        "            else unknown\n",
        "            for token in cleaned_sentence.split()\n",
        "        ]\n",
        "        tokenized_sentences.append(tokens)\n",
        "\n",
        "    return tokenized_sentences\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_folder = \"imdb_data/unsup\"\n",
        "sentences = load_imdb_unsup_sentences(imdb_folder)\n",
        "\n",
        "print(f\"Number of raw sentences loaded: {len(sentences)}\")\n",
        "print(f\"Example (first 2 sentences):\\n{sentences[:2]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-5469mMKcRP",
        "outputId": "c9e4c019-9096-4df4-a2f3-26d9502469ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of raw sentences loaded: 50000\n",
            "Example (first 2 sentences):\n",
            "['I saw this movie a while ago, awaiting a good and critical movie. I´m not afraid of a little violence in movies, but in this movie it just made no sense. Random useless violence all the time. There just wasn´t any goal. Maybe the director wanted to bring this feeling to the viewers, but that could have been done in much better ways. <nl>  <nl> The characters in this movie also have a complete lack of personality. The are flat characters who just commit violent stuff. <nl>  <nl> A ´not so good´ movie which stands in no comparison to American History X and Romperstomper.', \"Production house Amicus had a rich tradition in serving the so-called horror omnibuses. Long feature films telling three separate tales that mix humor and horror. The Monster Club was the last one and I can't really say I'm sorry for that. I didn't enjoy this film at all since it only has a few good moments and even those aren't highly memorable. Two absolute legends in the field provide the Monster Club with a silly wraparound story. John Carradine is a horror-author who donated a little bit of blood to vampire Vincent Price.this latter is so grateful for this that he takes Carradine to the 'Club' where the writer could find inspiration for upcoming horror stories. Let's start with the worst news first: the second story is really stupid and a complete waste of Donald Pleasance's horror talent. It's a very boring vampire-tale that's neither scary.nor funny. The first and last stories are watchable, yet the quality level never surpasses your average 'Tales from the Crypt' episode or something. The reasons why these fables are more or less interesting are because they introduce interesting variants on the common monsters. The protagonists are for example a Shadmock and a Humghoul! This last one is a result of sex between a ghoul and a human being! The gruesome make-up effects are limited and the direction is really crappy. Which is really surprising since Roy Ward Baker did a few outstanding directing jobs in the past. For instance, Dr. Jeckyll and Sister Hyde and Quartermass and the Pit for the Hammer Studio's. Vincent Price looks so tired and fed up that he almost becomes pitiful.yet he still manages to ramble a funny monologue near the end. The whole thing is completely ruined by bad (and too much) pop/rock music that form yet another connection between the stories. The only sequence that is really worth mentioning is a female stripper that goes all the way.and I do mean ALL the way.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(sentences) == 50000, \"Expected 50,000 sentences from the unsup folder.\""
      ],
      "metadata": {
        "id": "Qv0J6dGhIidP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "\n",
        "def split_data(sentences, test_split=0.1):\n",
        "    \"\"\"\n",
        "      shuffle the sentences\n",
        "      split them into train and test sets (first 1-test_split of the data is the training)\n",
        "      return the train and test sets\n",
        "    \"\"\"\n",
        "    shuffled_sentences = sentences[:]  # Copy the list to avoid modifying the original\n",
        "    random.shuffle(shuffled_sentences)  # Shuffle sentences randomly\n",
        "\n",
        "    split_idx = int(len(shuffled_sentences) * (1 - test_split))\n",
        "    train_sentences = shuffled_sentences[:split_idx]\n",
        "    test_sentences = shuffled_sentences[split_idx:]\n",
        "\n",
        "    return train_sentences, test_sentences\n"
      ],
      "metadata": {
        "id": "9hA3B8WEKAF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, test_sentences = split_data(sentences)\n",
        "\n",
        "print(f\"Number of training sentences: {len(train_sentences)}\")\n",
        "print(f\"Number of test sentences: {len(test_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfYWg45aKNzP",
        "outputId": "067ebee3-a11c-438c-f1f7-1e254081ce19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 45000\n",
            "Number of test sentences: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(train_sentences) == 45000, \"Expected 45,000 sentences for training.\"\n",
        "assert len(test_sentences) == 5000, \"Expected 5,000 sentences for testing.\"\n"
      ],
      "metadata": {
        "id": "i9Hh9ptkKS6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocabulary(train_sentences)\n",
        "tokenized_sentences = tokinize(train_sentences, vocab)\n",
        "\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Example tokens from first sentence: {tokenized_sentences[0][:200] if tokenized_sentences else 'No tokens loaded'} ...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI_q9qARKw_u",
        "outputId": "72a4dce7-fa64-4910-c3b3-ff96c5960607"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 161276\n",
            "Example tokens from first sentence: ['peruvian', 'writerdirector', 'josue', 'mendez', 'has', 'made', 'a', 'brave', 'little', 'low', 'budget', 'film', 'that', 'deals', 'with', 'a', 'subject', 'currently', 'burgeoning', 'our', 'hospitals', 'in', 'this', 'country', 'as', 'the', 'fallout', 'of', 'the', 'war', 'on', 'iraq', 'and', 'still', 'plagues', 'the', 'veterans', 'of', 'the', 'vietnam', 'war', 'post', 'traumatic', 'stress', 'syndrome', 'aka', 'battle', 'rattle', 'this', 'is', 'a', 'difficult', 'topic', 'to', 'dramatize', 'without', 'being', 'preachy', 'or', 'maudlin', 'but', 'mendez', 'has', 'succeeded', 'where', 'others', 'have', 'failed', 'nl', 'nl', 'santiago', 'pietro', 'sibille', 'is', 'a', '23yearold', 'retired', 'veteran', 'who', 'was', 'conscripted', 'at', 'age', '16', 'and', 'trained', 'to', 'be', 'a', 'killer', 'assigned', 'to', 'fighting', 'in', 'the', 'war', 'against', 'ecuador', 'against', 'terrorists', 'and', 'against', 'the', 'drug', 'mafia', 'he', 'returns', 'to', 'his', 'family', 'in', 'lima', 'a', 'damaged', 'broken', 'paranoid', 'misfit', 'who', 'tries', 'to', 'leave', 'his', 'military', 'past', 'behind', 'but', 'mentally', 'returns', 'to', 'it', 'as', 'the', 'only', 'time', 'he', 'felt', 'important', 'unable', 'to', 'work', 'he', 'finally', 'begins', 'to', 'drive', 'a', 'taxi', 'and', 'encounters', 'all', 'manner', 'of', 'passengers', 'wealthy', 'men', 'girls', 'on', 'the', 'party', 'circuit', 'disreputable', 'people', 'of', 'all', 'types', 'he', 'tries', 'desperately', 'to', 'adjust', 'to', 'the', 'postmilitary', 'life', 'but', 'find', 'his', 'family', 'in', 'shambles', 'a', 'wholly', 'dysfunctional', 'unit', 'to', 'which', 'he', 'can', 'no', 'longer', 'relate', 'how', 'he', 'finds', 'his', 'place', 'in', 'this', 'chaos', 'is'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(vocab) == 161276, \"Expected a vocabulary size of 171,591.\"\n",
        "assert len(tokenized_sentences) == 45000, \"Expected tokenized sentences count to match raw sentences.\"\n",
        "\n",
        "example = \"I love Natural language processing, and i want to be a great engineer.\"\n",
        "assert len(example) == 70, \"Example sentence length (in characters) does not match the expected 70.\"\n",
        "\n",
        "example_tokens = tokinize([example], vocab)[0]\n",
        "assert len(example_tokens) == 13, \"Token count for the example sentence does not match the expected 13.\"\n"
      ],
      "metadata": {
        "id": "9lbynIF5K6xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sentence(tokens, n):\n",
        "    padded = ['<s>'] * (n - 1) + tokens + ['</s>']\n",
        "    return padded\n",
        "\n",
        "\n",
        "def build_ngram_counts(tokenized_sentences, n):\n",
        "    \"\"\"\n",
        "    Builds n-gram counts and (n-1)-gram counts from the given tokenized sentences.\n",
        "    \"\"\"\n",
        "    ngram_counts = Counter()\n",
        "    context_counts = Counter()\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_sentence = pad_sentence(sentence, n)\n",
        "\n",
        "        for i in range(len(padded_sentence) - n + 1):\n",
        "            ngram = tuple(padded_sentence[i:i + n])\n",
        "            context = tuple(padded_sentence[i:i + n - 1])\n",
        "\n",
        "            ngram_counts[ngram] += 1\n",
        "            context_counts[context] += 1\n",
        "\n",
        "    return ngram_counts, context_counts\n",
        "\n",
        "\n",
        "def laplace_probability(ngram, ngram_counts, context_counts, vocab_size, alpha=1.0):\n",
        "    context = ngram[:-1]  # The (n-1)-gram context\n",
        "    ngram_count = ngram_counts.get(ngram, 0)\n",
        "    context_count = context_counts.get(context, 0)\n",
        "\n",
        "    probability = (ngram_count + alpha) / (context_count + alpha * vocab_size)\n",
        "    return probability\n"
      ],
      "metadata": {
        "id": "9bQ5K2ubNFhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 4\n",
        "ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n=n)\n",
        "print(f\"Number of bigrams: {len(ngram_counts)}\")\n",
        "print(f\"Number of contexts: {len(context_counts)}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFRligyRx_8",
        "outputId": "7f812fb9-54e9-4c42-89f6-c1d4e70dac26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of bigrams: 8808241\n",
            "Number of contexts: 6085736\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "from math import log, exp\n",
        "\n",
        "def predict_next_token(\n",
        "    context_tokens,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab,\n",
        "    n,\n",
        "    alpha=1.0,\n",
        "    top_k=5\n",
        "):\n",
        "\n",
        "    context = tuple(context_tokens[-(n-1):])  # Extract the last (n-1) tokens\n",
        "    candidates = []\n",
        "\n",
        "    for word in vocab:\n",
        "        ngram = context + (word,)\n",
        "        count_ngram = ngram_counts.get(ngram, 0)\n",
        "        count_context = context_counts.get(context, 0)\n",
        "        prob = (count_ngram + alpha) / (count_context + alpha * len(vocab))\n",
        "        candidates.append((word, prob))\n",
        "\n",
        "    # Sort candidates by probability (highest first)\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    return candidates[:top_k] if candidates else []  # Return top_k words\n",
        "\n",
        "def generate_text_with_limit(  start_tokens,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab,\n",
        "    n,\n",
        "    alpha=1.0,\n",
        "    max_length=20,\n",
        "    top_k=5\n",
        "):\n",
        "\n",
        "    generated = list(start_tokens)\n",
        "\n",
        "    for _ in range(max_length - len(start_tokens)):\n",
        "        next_word_candidates = predict_next_token(\n",
        "            generated, ngram_counts, context_counts, vocab, n, alpha, top_k\n",
        "        )\n",
        "\n",
        "        if not next_word_candidates:\n",
        "            break\n",
        "\n",
        "        # (avoids repetition)\n",
        "        words, probabilities = zip(*next_word_candidates)\n",
        "        next_word = random.choices(words, weights=probabilities, k=1)[0]\n",
        "\n",
        "        if next_word == \"</s>\":\n",
        "            break\n",
        "\n",
        "        generated.append(next_word)\n",
        "\n",
        "    return generated\n",
        "\n",
        "context = [\"i\", \"loved\"]\n",
        "generated_seq = generate_text_with_limit(\n",
        "    start_tokens=context,\n",
        "    ngram_counts=ngram_counts,\n",
        "    context_counts=context_counts,\n",
        "    vocab=vocab,\n",
        "    n=4,\n",
        "    alpha=1.0,\n",
        "    max_length=128\n",
        ")\n",
        "\n",
        "print(\"Generated Sequence:\", \" \".join(generated_seq))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1v4NzCsBMzTN",
        "outputId": "d4444aeb-3b8e-4c22-a503-953bb1ec04e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: i loved masterowner overstretch spiderloser battlegrounds masterowner masterowner battlegrounds overstretch battlegrounds masterowner blight spiderloser blight spiderloser battlegrounds battlegrounds masterowner spiderloser overstretch blight spiderloser masterowner battlegrounds masterowner battlegrounds spiderloser overstretch blight blight overstretch battlegrounds battlegrounds overstretch battlegrounds spiderloser battlegrounds battlegrounds spiderloser spiderloser overstretch masterowner blight battlegrounds blight battlegrounds overstretch overstretch blight masterowner masterowner masterowner overstretch masterowner battlegrounds spiderloser overstretch blight blight spiderloser blight battlegrounds spiderloser spiderloser overstretch overstretch overstretch blight blight battlegrounds blight overstretch masterowner spiderloser overstretch spiderloser masterowner overstretch blight blight spiderloser masterowner blight overstretch overstretch spiderloser masterowner overstretch spiderloser spiderloser spiderloser blight battlegrounds overstretch blight overstretch spiderloser overstretch battlegrounds spiderloser overstretch battlegrounds blight spiderloser battlegrounds blight spiderloser blight overstretch overstretch spiderloser battlegrounds spiderloser overstretch masterowner overstretch masterowner blight battlegrounds overstretch overstretch overstretch masterowner spiderloser overstretch blight masterowner\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "MggJtJ6nXZO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log, exp\n",
        "\n",
        "def calculate_perplexity(tokenized_sentences, ngram_counts, context_counts, vocab_size, n, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Calculates the perplexity of an n-gram model using the given formula.\n",
        "\n",
        "    Args:\n",
        "      tokenized_sentences: List of lists of tokens.\n",
        "      ngram_counts: Counter of n-grams.\n",
        "      context_counts: Counter of (n-1)-grams.\n",
        "      vocab_size: Size of the vocabulary.\n",
        "      n: n-gram order.\n",
        "      alpha: Laplace smoothing parameter.\n",
        "\n",
        "    Returns:\n",
        "      A float representing the perplexity.\n",
        "    \"\"\"\n",
        "    log_prob_sum = 0.0\n",
        "    token_count = 0\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_sentence = pad_sentence(sentence, n)  # Pad sentence with <s> and </s>\n",
        "        for i in range(len(padded_sentence) - n + 1):\n",
        "            ngram = tuple(padded_sentence[i : i + n])\n",
        "            context = tuple(padded_sentence[i : i + n - 1])\n",
        "\n",
        "\n",
        "            ngram_freq = ngram_counts[ngram] + alpha\n",
        "            context_freq = context_counts[context] + (alpha * vocab_size)\n",
        "            prob = ngram_freq / context_freq\n",
        "\n",
        "            log_prob_sum += log(prob)\n",
        "            token_count += 1\n",
        "    perplexity = exp(-log_prob_sum / token_count) if token_count > 0 else float(\"inf\")\n",
        "    return perplexity\n",
        "\n",
        "perplexity = calculate_perplexity(\n",
        "    tokenized_sentences,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab_size=len(vocab),\n",
        "    n=4,\n",
        "    alpha=1.0\n",
        ")\n",
        "print(f\"Perplexity: {perplexity}\")\n"
      ],
      "metadata": {
        "id": "_LN_xGAcGPnt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e656b834-c0e3-4cf9-dd47-5a671e3ca5f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 56621.75317998516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis**\n",
        "n=2 Perplexity=2931.987399\n",
        "\n",
        "n=3 Perplexity: 24666.424455167256\n",
        "\n",
        "n=4 Perplexity: 56621.75317998516"
      ],
      "metadata": {
        "id": "__ExBYFpgj30"
      }
    }
  ]
}