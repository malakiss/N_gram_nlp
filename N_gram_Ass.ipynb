{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malakiss/N_gram_nlp/blob/main/N_gram_Ass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Process the IMDB Dataset"
      ],
      "metadata": {
        "id": "cD9ZRNeIJ_Br"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet gdown\n",
        "\n",
        "# 1. Download the zipped IMDB dataset from Drive\n",
        "# this is the unsup part of https://ai.stanford.edu/~amaas/data/sentiment/\n",
        "\n",
        "!gdown \"https://drive.google.com/uc?id=1PjJ5cop0pT6tcEw9-ZUstVMujx-o-QTB\" -O imdb_dataset.zip\n",
        "\n",
        "# 2. Unzip the downloaded file\n",
        "!unzip -q imdb_dataset.zip -d imdb_data\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0olLNujvMbg8",
        "outputId": "bf3f8ef2-68f2-4e40-d12a-86ba55389ffd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1PjJ5cop0pT6tcEw9-ZUstVMujx-o-QTB\n",
            "From (redirected): https://drive.google.com/uc?id=1PjJ5cop0pT6tcEw9-ZUstVMujx-o-QTB&confirm=t&uuid=5d11bccf-25e5-4727-aa42-0671d8a0eb12\n",
            "To: /content/imdb_dataset.zip\n",
            "100% 44.7M/44.7M [00:00<00:00, 120MB/s]\n",
            "replace imdb_data/unsup/0_0.txt? [y]es, [n]o, [A]ll, [N]one, [r]ename: A\n",
            "A\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import string\n",
        "import random\n",
        "from collections import defaultdict, Counter\n",
        "import math\n",
        "from math import log, exp\n"
      ],
      "metadata": {
        "id": "siDkX864cazJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G1K-8ybJJ3Cl"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "\n",
        "def load_imdb_unsup_sentences(folder_path):\n",
        "    \"\"\"\n",
        "    Loads text files from the IMDB 'unsup' (unsupervised) folder.\n",
        "    - Reads all `.txt` files from the given folder.\n",
        "    - Splits text by newline, strips each line, and returns a list of raw lines.\n",
        "    - Replaces <br /> tags with a special token <nl>.\n",
        "    \"\"\"\n",
        "    all_sentences = []\n",
        "\n",
        "    for file_name in os.listdir(folder_path):\n",
        "        file_path = os.path.join(folder_path, file_name)\n",
        "        if os.path.isfile(file_path) and file_name.endswith('.txt'):\n",
        "            with open(file_path, 'r', encoding='utf-8') as f:\n",
        "                for line in f:\n",
        "                    line = line.strip()\n",
        "                    if line:\n",
        "                        line = re.sub(r\"<br\\s*/?>\", \" <nl> \", line)  # Replace <br /> with <nl>\n",
        "                        all_sentences.append(line)\n",
        "\n",
        "    return all_sentences\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    \"\"\"\n",
        "    Removes punctuation from the text while preserving <nl> tokens.\n",
        "    Also removes apostrophes and stopwords.\n",
        "    \"\"\"\n",
        "    text = re.sub(r\"<br\\s*/?>\", \" <nl> \", text)  # Ensure <br /> becomes <nl>\n",
        "    regex_pattern = f\"[{re.escape(string.punctuation)}]\"\n",
        "    text = re.sub(regex_pattern, \"\", text)\n",
        "    processed_words = []\n",
        "    for word in text.split():\n",
        "          processed_words.append(word)\n",
        "\n",
        "    return \" \".join(processed_words)\n",
        "\n",
        "def build_vocabulary(sentences):\n",
        "    vocab = set()\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = remove_punctuation(sentence.lower())  # Lowercase & clean\n",
        "        tokens = cleaned_sentence.split()\n",
        "        vocab.update(tokens)\n",
        "    return vocab\n",
        "def tokinize(sentences, vocab, unknown=\"<UNK>\"):\n",
        "    tokenized_sentences = []\n",
        "    for sentence in sentences:\n",
        "        cleaned_sentence = remove_punctuation(sentence.lower())  # Lowercase & clean\n",
        "        tokens = [\n",
        "            token\n",
        "            if token in vocab\n",
        "            else unknown\n",
        "            for token in cleaned_sentence.split()\n",
        "        ]\n",
        "        tokenized_sentences.append(tokens)\n",
        "    return tokenized_sentences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "imdb_folder = \"imdb_data/unsup\"\n",
        "sentences = load_imdb_unsup_sentences(imdb_folder)\n",
        "\n",
        "print(f\"Number of raw sentences loaded: {len(sentences)}\")\n",
        "print(f\"Example (first 2 sentences):\\n{sentences[:2]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y-5469mMKcRP",
        "outputId": "c9fa4476-88de-4ba3-a605-c9aaf7704c3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of raw sentences loaded: 50000\n",
            "Example (first 2 sentences):\n",
            "[\"For the particular movie fan, The Beat That My Heart Skipped, is a slice of intensity, wonder, and subtlety that can only come from Europe. The director/co-writer, Jacques Audiard, has taken a film previously made by James Toback called Fingers, starring Harvey Keitel in the role now occupied by Romain Duris, and made it his own. If I had seen the original version I would make a couple of comparisons to it (at the least, for those who didn't see the original the remake makes you want to check out the original, if only for the acting appeal of Keitel). However I did think about another wonderful French film in the vein of this film- Francois Truffaut's Shoot the Piano Player. <nl>  <nl> While Truffaut's film is a little more concerned about the lead's relationship(s) with women, I felt a kind of connection between the material of the two pieces- sometimes intense, usually lyrical, tales of a person trying to find what fits more for them, the more criminal side, or the artistic side. And, much like Truffaut and his other New-Wave counterparts, Audiard successfully takes an American formula picture and forms it well into a French setting. <nl>  <nl> There are a few reasons to recommend the movie, one would be for the music, which gives repeated but specific renditions of a Bach tune. Another would be just for the technical-side, which is well-done in hand-held, neo-noir style by Stephane Fontaine. Another could even just be for how Audiard tells his story, or sometimes doesn't tell it: a couple of times mid-way through the film, I wondered if the story of this character would 'go' anywhere, which can either make or break a film of this kind. It pleasantly (or intensely) did, bringing a catharsis for a viewer by the final scenes. <nl>  <nl> But likely for most the prominent reason would be for the realistic acting, in particular by its star Duris. As I said, I can't make comparisons between a heavyweight like Keitel and Duris (whom I've never seen in a film before this), but on his own terms Duris creates his character believably. It's at times a complex character, or sometimes not- he has that kind of attitude and face where you don't know whether he's really a 'street-level' guy or more straight laced. The split that is also apparent in the character's parents, one a classic pianist who's passed on (the mother), and the other a more criminal-type of a father, also gives the film an added boost of psychological energy. The lead in this film, much like with the storytelling (or lack of it), dictates how it may turn out. <nl>  <nl> In the end, Audiard and Duris make it compelling enough for the film to be about him, his conflicts, his lusts, his music. It's a wonderful movie that seems to have passed under the radar (it's in only a few theaters around the area) amid other independent summer fare, but if you're an enthusiast of character-driven thrillers that give a bitter-sweet edge, it's a must-see.\", 'I\\'ve come to expect very little out of the SciFi channel\\'s made for TV movies, but this ranks tight alongside of \"Babylon 5: Legend of the Rangers\" as am egregious example of how SciFi\\'s clueless network suits can prostitute a great work with an appallingly bad sequel. <nl>  <nl> I\\'m a great fan of the original \"Lake Placid\". David E. Kelly\\'s writing in the original is among the sharpest of any film in memory. It had a delightfully witty script and excellent performances by talented actors. The characters were well textured without a stereotype among them. <nl>  <nl> By comparison, the script of this film is drek. Thre is almost no wit evident. It\\'s trite and formulaic. The characters are all 2-dimensional stereotypes from central casting. The \"special\" effects were everything I\\'ve come to expect from SciFi channel movies - ham-fisted and amateurish. <nl>  <nl> I might have given this film a rating of 2 or 3, but for potentially tainting the reputation of the original, I give it a 1 - but only because there\\'s no option to give it a zero!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(sentences) == 50000, \"Expected 50,000 sentences from the unsup folder.\""
      ],
      "metadata": {
        "id": "Qv0J6dGhIidP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(42)\n",
        "\n",
        "def split_data(sentences, test_split=0.1):\n",
        "    \"\"\"\n",
        "      shuffle the sentences\n",
        "      split them into train and test sets (first 1-test_split of the data is the training)\n",
        "      return the train and test sets\n",
        "    \"\"\"\n",
        "    shuffled_sentences = sentences[:]  # Copy the list to avoid modifying the original\n",
        "    random.shuffle(shuffled_sentences)  # Shuffle sentences randomly\n",
        "\n",
        "    split_idx = int(len(shuffled_sentences) * (1 - test_split))\n",
        "    train_sentences = shuffled_sentences[:split_idx]\n",
        "    test_sentences = shuffled_sentences[split_idx:]\n",
        "\n",
        "    return train_sentences, test_sentences\n"
      ],
      "metadata": {
        "id": "9hA3B8WEKAF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, test_sentences = split_data(sentences)\n",
        "\n",
        "print(f\"Number of training sentences: {len(train_sentences)}\")\n",
        "print(f\"Number of test sentences: {len(test_sentences)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfYWg45aKNzP",
        "outputId": "8873a14e-51ee-4400-9842-5813f9591bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training sentences: 45000\n",
            "Number of test sentences: 5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(train_sentences) == 45000, \"Expected 45,000 sentences for training.\"\n",
        "assert len(test_sentences) == 5000, \"Expected 5,000 sentences for testing.\"\n"
      ],
      "metadata": {
        "id": "i9Hh9ptkKS6Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = build_vocabulary(train_sentences)\n",
        "tokenized_sentences = tokinize(train_sentences, vocab)\n",
        "test_tokenized = tokinize(test_sentences, vocab)\n",
        "print(f\"Vocabulary size: {len(vocab)}\")\n",
        "print(f\"Example tokens from first sentence: {tokenized_sentences[0][:200] if tokenized_sentences else 'No tokens loaded'} ...\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZI_q9qARKw_u",
        "outputId": "bf92094b-325c-4892-abd9-eea47c029272"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 161741\n",
            "Example tokens from first sentence: ['i', 'saw', 'this', 'recently', 'after', 'seeing', 'terrance', 'davies', 'lovely', 'the', 'house', 'nl', 'nl', 'of', 'mirth', 'and', 'was', 'not', 'disappointed', 'actually', 'they', 'make', 'a', 'nice', 'little', 'nl', 'nl', 'edith', 'wharton', 'weekend', 'if', 'you', 'have', 'a', 'lot', 'of', 'time', 'on', 'your', 'hands', 'nl', 'nl', 'and', 'want', 'to', 'feel', 'so', 'good', 'about', 'your', 'own', 'life', 'and', 'choices', 'that', 'you', 'nl', 'nl', 'want', 'to', 'shoot', 'yourself', 'and', 'i', 'mean', 'that', 'in', 'a', 'good', 'way', 'this', 'movie', 'nl', 'nl', 'is', 'beautifully', 'crafted', 'and', 'features', 'winona', 'ryders', 'best', 'nl', 'nl', 'performance', 'since', 'little', 'women', 'daniel', 'day', 'lewis', 'has', 'the', 'nl', 'nl', 'thankless', 'edith', 'wharton', 'male', 'lead', 'role', 'she', 'wrote', 'best', 'for', 'nl', 'nl', 'women', 'and', 'michelle', 'pfeiffer', 'is', 'that', 'obscure', 'object', 'of', 'desire', 'this', 'nl', 'nl', 'film', 'doesnt', 'have', 'the', 'bite', 'and', 'bile', 'that', 'most', 'scorsese', 'films', 'have', 'nl', 'nl', 'but', 'its', 'well', 'worth', 'the', 'trip'] ...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "assert len(vocab) == 161741, \"Expected a vocabulary size of 171,591.\"\n",
        "assert len(tokenized_sentences) == 45000, \"Expected tokenized sentences count to match raw sentences.\"\n",
        "\n",
        "example = \"I love Natural language processing, and i want to be a great engineer.\"\n",
        "assert len(example) == 70, \"Example sentence length (in characters) does not match the expected 70.\"\n",
        "\n",
        "example_tokens = tokinize([example], vocab)[0]\n",
        "assert len(example_tokens) == 13, \"Token count for the example sentence does not match the expected 13.\"\n"
      ],
      "metadata": {
        "id": "9lbynIF5K6xJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pad_sentence(tokens, n):\n",
        "    padded = ['<s>'] * (n - 1) + tokens + ['</s>']\n",
        "    return padded\n",
        "def build_ngram_counts(tokenized_sentences, n):\n",
        "    \"\"\"\n",
        "    Builds n-gram counts and (n-1)-gram counts from the given tokenized sentences.\n",
        "    \"\"\"\n",
        "    ngram_counts = Counter()\n",
        "    context_counts = Counter()\n",
        "\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_sentence = pad_sentence(sentence, n)\n",
        "        for i in range(len(padded_sentence) - n + 1):\n",
        "            ngram = tuple(padded_sentence[i:i + n])\n",
        "            context = tuple(padded_sentence[i:i + n - 1])\n",
        "            ngram_counts[ngram] += 1\n",
        "            context_counts[context] += 1\n",
        "    return ngram_counts, context_counts\n",
        "\n",
        "\n",
        "def laplace_probability(ngram, ngram_counts, context_counts, vocab_size, alpha=1.0):\n",
        "    context = ngram[:-1]\n",
        "    ngram_count = ngram_counts.get(ngram, 0)\n",
        "    context_count = context_counts.get(context, 0)\n",
        "    probability = (ngram_count + alpha) / (context_count + alpha * vocab_size)\n",
        "    return probability\n"
      ],
      "metadata": {
        "id": "9bQ5K2ubNFhD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 1\n",
        "ngram_counts, context_counts = build_ngram_counts(tokenized_sentences, n=n)\n",
        "print(f\"Number of bigrams: {len(ngram_counts)}\")\n",
        "print(f\"Number of contexts: {len(context_counts)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgFRligyRx_8",
        "outputId": "1da70a54-a08b-4269-e393-36e89019b651"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of bigrams: 161742\n",
            "Number of contexts: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def predict_next_token(context_tokens, ngram_counts, context_counts, vocab, n, alpha=1.0, top_k=5):\n",
        "    context = tuple(context_tokens[-(n-1):])  # Extract the last (n-1) tokens as context\n",
        "    candidates = [(token, laplace_probability(context + (token,), ngram_counts, context_counts, len(vocab), alpha))\n",
        "                  for token in vocab]\n",
        "\n",
        "    # Sort candidates by probability in descending order\n",
        "    candidates.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "    # Select top_k candidates\n",
        "    top_candidates = candidates[:top_k]\n",
        "\n",
        "    # Normalize probabilities\n",
        "    total_prob = sum(prob for _, prob in top_candidates)\n",
        "    if total_prob > 0:\n",
        "        top_candidates = [(token, prob / total_prob) for token, prob in top_candidates]\n",
        "\n",
        "    # Perform weighted random sampling from top_k candidates\n",
        "    tokens, probs = zip(*top_candidates)\n",
        "    next_token = random.choices(tokens, weights=probs, k=1)[0]\n",
        "\n",
        "    return [(next_token, dict(top_candidates).get(next_token, 0.0))]  # Return sampled token with its actual probability\n",
        "def generate_text_with_limit(start_tokens,ngram_counts,context_counts,vocab,n,alpha=1.0, max_length=20):\n",
        "    generated = list(start_tokens)\n",
        "    # Pad the initial context with <s> tokens if it's shorter than n-1\n",
        "    if len(generated) < n - 1:\n",
        "        generated =pad_sentence(generated, n)\n",
        "    while len(generated) < max_length:\n",
        "        next_token_candidates = predict_next_token(generated, ngram_counts,context_counts,vocab,n=n, alpha=alpha,top_k=5)\n",
        "        if not next_token_candidates:\n",
        "            break\n",
        "        next_token = next_token_candidates[0][0]\n",
        "        generated.append(next_token)\n",
        "        if next_token == \"</s>\":\n",
        "            break\n",
        "    return generated\n",
        "context = [\"i\", \"love\"]\n",
        "generated_seq = generate_text_with_limit(start_tokens=context,ngram_counts=ngram_counts,context_counts=context_counts,vocab=vocab,n=1,alpha=1.0, max_length=128)\n",
        "print(\"Generated Sequence:\", generated_seq)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_BfIuBdvzp2",
        "outputId": "a84269a5-6e27-4add-9490-c674477afb8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Sequence: ['i', 'love', 'forementioned', 'fulfiling', 'economising', 'economising', 'economising', 'forementioned', 'economising', 'forementioned', 'comavision', 'comavision', 'forementioned', 'forementioned', 'forementioned', 'economising', 'fulfiling', 'littlemy', 'fulfiling', 'comavision', 'forementioned', 'forementioned', 'forementioned', 'economising', 'forementioned', 'fulfiling', 'forementioned', 'fulfiling', 'comavision', 'comavision', 'forementioned', 'littlemy', 'comavision', 'comavision', 'forementioned', 'economising', 'economising', 'comavision', 'economising', 'economising', 'economising', 'comavision', 'littlemy', 'comavision', 'economising', 'economising', 'comavision', 'forementioned', 'fulfiling', 'littlemy', 'forementioned', 'fulfiling', 'comavision', 'economising', 'fulfiling', 'fulfiling', 'littlemy', 'littlemy', 'economising', 'economising', 'fulfiling', 'comavision', 'littlemy', 'littlemy', 'forementioned', 'fulfiling', 'fulfiling', 'littlemy', 'comavision', 'economising', 'fulfiling', 'forementioned', 'littlemy', 'comavision', 'forementioned', 'comavision', 'forementioned', 'forementioned', 'littlemy', 'littlemy', 'forementioned', 'fulfiling', 'comavision', 'littlemy', 'forementioned', 'comavision', 'fulfiling', 'forementioned', 'comavision', 'fulfiling', 'forementioned', 'forementioned', 'fulfiling', 'littlemy', 'fulfiling', 'fulfiling', 'forementioned', 'forementioned', 'littlemy', 'fulfiling', 'forementioned', 'economising', 'economising', 'comavision', 'fulfiling', 'comavision', 'littlemy', 'economising', 'economising', 'comavision', 'littlemy', 'littlemy', 'comavision', 'littlemy', 'fulfiling', 'fulfiling', 'littlemy', 'comavision', 'comavision', 'fulfiling', 'littlemy', 'littlemy', 'littlemy', 'comavision', 'economising', 'forementioned', 'comavision', 'economising']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_perplexity(tokenized_sentences, ngram_counts, context_counts, vocab_size, n, alpha=1.0):\n",
        "    log_prob_sum = 0.0\n",
        "    token_count = 0\n",
        "    for sentence in tokenized_sentences:\n",
        "        padded_sentence = pad_sentence(sentence, n)\n",
        "        for i in range(len(padded_sentence) - n + 1):\n",
        "            ngram = tuple(padded_sentence[i : i + n])\n",
        "            prob = laplace_probability(ngram, ngram_counts, context_counts, vocab_size, alpha)\n",
        "            log_prob_sum += log(prob)\n",
        "            token_count += 1\n",
        "    perplexity = exp(-log_prob_sum / token_count)\n",
        "    return perplexity\n",
        "perplexity = calculate_perplexity(\n",
        "    test_tokenized,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab_size=len(vocab),\n",
        "    n=1,\n",
        "    alpha=1\n",
        ")\n",
        "print(f\"Perplexity: {perplexity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LaDVqLSvvhnf",
        "outputId": "16d65e65-4c7d-4b4e-8c3c-35e0afa4fb6e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity: 1205.8826669851508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import log, exp\n",
        "\n",
        "def calculate_sentence_perplexity(sentence, ngram_counts, context_counts, vocab_size, n, alpha=1.0):\n",
        "    \"\"\"\n",
        "    Calculate the perplexity for a single sentence using an n-gram model.\n",
        "    \"\"\"\n",
        "    log_prob_sum = 0.0\n",
        "    token_count = 0\n",
        "\n",
        "    # Pad the sentence with <s> and </s>\n",
        "    padded_sentence = pad_sentence(sentence, n)\n",
        "\n",
        "    for i in range(len(padded_sentence) - n + 1):\n",
        "        ngram = tuple(padded_sentence[i : i + n])\n",
        "        context = tuple(padded_sentence[i : i + n - 1])\n",
        "\n",
        "        # Get n-gram and context frequencies (with Laplace smoothing)\n",
        "        ngram_freq = ngram_counts.get(ngram, 0) + alpha\n",
        "        context_freq = context_counts.get(context, 0) + (alpha * vocab_size)\n",
        "\n",
        "        prob = ngram_freq / context_freq\n",
        "        log_prob_sum += log(prob)\n",
        "        token_count += 1\n",
        "\n",
        "    # Compute perplexity\n",
        "    perplexity = exp(-log_prob_sum / token_count)\n",
        "    return perplexity\n",
        "\n",
        "# Define your test sentence\n",
        "test_sentence = [\"i\", \"loved\", \"this\", \"movie\"]\n",
        "\n",
        "# Compute perplexity for the sentence\n",
        "sentence_perplexity = calculate_sentence_perplexity(\n",
        "    test_sentence,\n",
        "    ngram_counts,\n",
        "    context_counts,\n",
        "    vocab_size=len(vocab),\n",
        "    n=1,\n",
        "    alpha=1.0\n",
        ")\n",
        "\n",
        "print(f\"Perplexity for sentence 'i loved this movie': {sentence_perplexity}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NSxCaGqWqeyd",
        "outputId": "02379d06-7cbe-4da0-8e7f-69a7fefd9648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Perplexity for sentence 'i loved this movie': 252.000291831992\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Analysis on Test Set**\n",
        "n=2 Perplexity: 3410.0383848243023\n",
        "\n",
        "n=3 Perplexity: 37117.2128829916\n",
        "\n",
        "n=4 Perplexity: 101534.96229645389"
      ],
      "metadata": {
        "id": "__ExBYFpgj30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis on Text\n",
        "n=1 Perplexity: 252.000291831992\n",
        "\n",
        "n=2 Perplexity: 98.2766219854523\n",
        "\n",
        "n=3 Perplexity: 425.8483425242998\n",
        "\n",
        "n=4 Perplexity: 1864.242523029493"
      ],
      "metadata": {
        "id": "DVHD3uLcUkbP"
      }
    }
  ]
}